\chapter{Podsumowanie i wnioski}
\label{podsumowanie.wnioski}

W trakcie realizacji tego projektu pojawiało się wiele problemów, które należało rozwiązać. Pomijając przegląd literatury, następnym krokiem było odpowiednie przygotowanie danych. Przede wszystkim konieczne było sprowadzenie zdjęć do tej samej skali oraz przybliżenia, a także usunięcie z nich niepotrzebnych elementów (jak rozmiar skali). Dodatkowo sztucznie wygenerowano kolejne dane za pomocą technik augmentacji, które mogłyby się przydać w przyszłości. Po obsłużeniu prac związanych z samymi danymi można już było przejść do poważniejszych zagadnień, które były tematem tej pracy. Idąc chronologicznie, pierwszym problemem była klasyfikacja struktur. Pierwszym przebadanym podejściem była uogólniona transformata Hougha, która to metoda służy do znajdowania na obrazie zapytania kształtów przedstawionych na obrazie referencyjnym. Co prawda, po kilku modyfikacjach procedury, wizualnie wyniki wyglądały świetnie (rozdz. \ref{hough}), aczkolwiek było to podejście mało praktyczne, gdyż dla każdego zdjęcia mikrostruktury potrzebowalibyśmy co najmniej sześć obrazów referencyjnych, a potencjalnie nawet więcej, gdyż w rzeczywistości struktury na zdjęciach nieco się różnią od tych wskazanych w normie (rys. \ref{fig:mesh14}). 
Mimo iż ta metoda była już blisko rozwiązania, to tak na prawdę dopiero kolejne przebadane metody spełniły nasze oczekiwania, a mianowicie algorytmy z biblioteki \href{https://opencv.org/}{\ita{OpenCV}}, takie jak wygładzanie, progowanie, a przede wszystkim metoda wykrywania krawędzi Canny'ego \cite{Canny86}. Dzięki tym metodom stało się możliwe wycinanie praktycznie wszystkich struktur, a następnie ręczne ich klasyfikowanie w celu stworzenia nowej bazy danych. Wyniki, jakie udało się osiągnąć (najlepszy wynik to $82.2\%$ dokładności) zostały zaprezentowane w rozdz. \ref{klasyfikacja.struktur}. Wyniki wizualnie wyglądają znakomicie, aczkolwiek weryfikując je za pomocą tablicy pomyłek (rys. \ref{fig:mesh29}) okazuje się, że występują lekkie zakłamania. Przede wszystkim wyniki dla pięciu z siedmiu klas wyglądają przyzwoicie, natomiast dla pozostałych dwóch klas (II oraz IV) są nieco gorsze. Przeprowadzono wiele testów i wszystko niestety wskazuje na to, iż za pomocą tych danych nie można osiągnąć już wiele lepszych wyników.

Następnie, w rozdz. \ref{Ocena jakości odlewów} przedstawiono badania, które były głównym tematem niniejszej pracy. W celu klasyfikacji binarnej wytrzymałości na rozciąganie odlewów wykorzystano metody zaprezentowane w poprzednich rozdziałach. Aczkolwiek pierwszą przebadaną metodą były momenty Hu oraz tekstury Haralicka (rozdz. \ref{sec:hu_haralick}) , co jak mogłoby się wydawać są idealnym narzędziem do tego celu, jeśli tylko struktury obecne na zdjęciach mają wpływ na wytrzymałość. Dodatkowo wykorzystano klasyczne modele klasyfikujące. Niestety, mimo iż wyniki były zróżnicowane, od niskich ($55\%$ dokładności) do wysokich ($70\%$ dokładności) to wciąż nie są to wyniki w pełni satysfakcjonujące. Następnie zostały przetestowane klasyczne klasyfikatory z wykorzystaniem wygenerowanych przez autora danych oraz klasycznych klasyfikatorów (rozdz. \ref{Ocena jakości odlewów}). Jest to najbardziej obszerny punkt badań, jednakże wyniki zostały zbiorczo przedstawione w rozdz. \ref{structures.summary}. Jak się okazało, jest to bardzo skuteczna metoda, gdyż udało się osiągnąć skuteczność na poziomie aż $83.4\%$ dokładności! W dodatku są to metody wysoce interpretowalne, przez co mogłyby służyć jako pomoc dla specjalistów, którzy za pomocą zdjęć chcieliby ocenić wytrzymałość danego metariału. Zaś w ostatnich testach (rozdz. \ref{binary.ann.with.structures}) przebadano skuteczność bardzo popularnych obecnie sieci neuronowych (w szczególności \ita{VGG19}), a także połączenie sieci neuronowych z klasycznymi klasyfikatorami (rozdz. \ref{binary.ann.with.structures}). Jak się jednak okazało, nie udało się osiągnąć lepszych wyników, zaledwie dorównując najlepszym otrzymanym wynikom do tej pory. Aczkolwiek wiemy, że sieci neuronowe sprawdzają się znacznie lepiej dla dużej liczby danych, co tutaj było lekkim problemem (opisane w dalszej części).

Jako dalszą pracę nad tym tematem można uznać konieczność lepszego przygotowania danych uczących (sześć kształtów z rysunku \ref{fig:mesh14}), na przykład przy pomocy specjalistów w tej dziedzinie. Przede wszystkim najważniejsze jest, aby w danej klasie znajdowały się możliwie tylko poprawne przykłady. Natomiast problemem niższej rangi (aczkolwiek w celu otrzymania jak najlepszych wyników również ważnym) jest zbalansowanie klas. Dodatkowo autor wybrał zaledwie ułamek z wyciętych struktur (około 1500), gdyż po sprowadzeniu zdjęć do tej samej skali (tzn. zdjęcia z przybliżeniem 100x sprowadzono do przybliżenia 500x, tym samym uzyskując z takiego pojedynczego zdjęcia kolejnych 25) jest ich aż 4797, z czego z każdego takiego zdjęcia jest wycinanych średnio 20 struktur, a więc liczba możliwych struktur uzyskanych z tego zbioru może być nawet większa, niż 100 000 (słownie: sto tysięcy), a więc wykorzystano około $1.5\%$ dostępnych danych. Stąd widać, że w kwestii danych jest jeszcze sporo miejsca na poprawę. Natomiast widzimy, że zarówno modele klasyczne jak i sieci neuronowe uzyskały przyzwoite skuteczności. W szczególności na ten moment uzasadnione wydaje się wykorzystanie modeli klasycznych (które de facto pośrednio również korzystają z efektów sieci VGG19), które mogłyby posłużyć ekspertom w tej dziedzinie jako pomoc w uzasadnieniu decyzji co do wytrzymałości danego materiału. Jeśli chodzi zaś o sieci neuronowe, tutaj konieczne wydaje się uzbieranie większej ilości danych i powtórzenie podobnych testów w celu wykazania, że mogą one osiągać jeszcze lepsze wyniki. 